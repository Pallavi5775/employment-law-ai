{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Autoloader Ingest\n",
    "\n",
    "Placeholder notebook. Use Databricks Autoloader to ingest employment contracts from Blob Storage into Delta Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb372c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, current_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, BinaryType, StringType\n",
    "\n",
    "raw_path = \"abfss://contracts@ragstorage4122025.dfs.core.windows.net/\"\n",
    "bronze_path = \"abfss://bronze@ragstorage4122025.dfs.core.windows.net/\"\n",
    "checkpoint = \"dbfs:/checkpoints/contracts/bronze/\"\n",
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"binary\")\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint + \"/schema\")\n",
    "        .load(raw_path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"filename\", input_file_name()) \\\n",
    "       .withColumn(\"ingest_timestamp\", current_timestamp())\n",
    "\n",
    "(\n",
    "    df.writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", checkpoint)\n",
    "      .trigger(availableNow=True)      # or continuous=true\n",
    "      .outputMode(\"append\")\n",
    "      .start(bronze_path)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
