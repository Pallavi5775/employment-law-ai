{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diagnostic Pipeline Checker (Databricks + Local hybrid)\n",
        "\n",
        "This notebook checks each layer of the Employment Contract ETL pipeline (Bronze → Silver Raw → Silver Clean → Gold) and reports issues preventing downstream training. It **autodetects** whether it's running inside Databricks (Spark available) or locally, and adapts accordingly.\n",
        "\n",
        "It will:\n",
        "- Verify presence and row counts for Bronze (raw files), Silver Raw (ocr text), Silver Clean (cleaned text), Gold (clauses, features)\n",
        "- Show sample rows\n",
        "- Detect empty tables, missing columns, or pipeline breakage\n",
        "- Provide actionable remediation tips\n",
        "\n",
        "## How to run\n",
        "- **Databricks**: Upload this notebook to Databricks Repos and run the cells. Set the widget paths or let defaults run.\n",
        "- **Local**: Install dependencies (`pandas`, `pyarrow`, optionally `pyspark` if you want to read delta via Spark locally). Then run in Jupyter/VSCode. Provide local paths (parquet/txt) via configuration below.\n",
        "\n",
        "**Created:** 2025-12-07T21:15:51.326546\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (edit before running)\n",
        "import os\n",
        "RUN_MODE = 'DATABRICKS'\n",
        "# Default DBFS / ADLS paths (used in Databricks)\n",
        "raw_path = \"abfss://contracts@ragstorage4122025.dfs.core.windows.net/\"\n",
        "bronze_path = \"abfss://bronze@ragstorage4122025.dfs.core.windows.net/\"\n",
        "checkpoint = \"dbfs:/checkpoints/contracts/bronze/\"\n",
        "silver_path = \"abfss://silver@ragstorage4122025.dfs.core.windows.net/\"\n",
        "gold_path = \"abfss://gold@ragstorage4122025.dfs.core.windows.net/\"\n",
        "\n",
        "\n",
        "\n",
        "# Local fallback paths (if running locally put your exported parquet files here)\n",
        "LOCAL_BRONZE_PATH = './data/bronze/'\n",
        "LOCAL_SILVER_RAW_PATH = './data/silver_raw/'\n",
        "LOCAL_SILVER_CLEAN_PATH = './data/silver_clean/'\n",
        "LOCAL_GOLD_PATH = './data/gold/'\n",
        "print('RUN_MODE:', RUN_MODE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: detect Databricks (Spark available) and prepare readers\n",
        "def in_databricks():\n",
        "    try:\n",
        "        # dbutils is available in Databricks\n",
        "        import dbutils\n",
        "        return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        # SparkSession via pyspark.sql in Databricks\n",
        "        from pyspark.sql import SparkSession\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "IS_DATABRICKS = False\n",
        "if RUN_MODE == 'AUTO':\n",
        "    IS_DATABRICKS = in_databricks()\n",
        "elif RUN_MODE == 'DATABRICKS':\n",
        "    IS_DATABRICKS = True\n",
        "elif RUN_MODE == 'LOCAL':\n",
        "    IS_DATABRICKS = False\n",
        "print('IS_DATABRICKS:', IS_DATABRICKS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Readers for Delta (Databricks) and Parquet/CSV locally\n",
        "def read_delta_table(path):\n",
        "    # expects Databricks environment with spark available\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    print('Reading Delta table from', path)\n",
        "    try:\n",
        "        df = spark.read.format('delta').load(path)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print('Delta read failed:', e)\n",
        "        # try reading path as parquet\n",
        "        try:\n",
        "            df = spark.read.parquet(path)\n",
        "            return df\n",
        "        except Exception as e2:\n",
        "            print('Parquet read also failed:', e2)\n",
        "            raise\n",
        "\n",
        "def read_parquet_local(path):\n",
        "    import pandas as pd\n",
        "    import glob\n",
        "    files = []\n",
        "    if os.path.isdir(path):\n",
        "        files = glob.glob(os.path.join(path, '*.parquet'))\n",
        "    elif os.path.isfile(path) and path.endswith('.parquet'):\n",
        "        files = [path]\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f'No parquet files found at {path}')\n",
        "    dfs = [pd.read_parquet(f) for f in files]\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def read_text_files_local(path):\n",
        "    import glob\n",
        "    texts = []\n",
        "    if os.path.isdir(path):\n",
        "        for f in glob.glob(os.path.join(path, '*.txt')):\n",
        "            with open(f, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "                texts.append({'filename': os.path.basename(f), 'text': fh.read()})\n",
        "    else:\n",
        "        raise FileNotFoundError(f'Path not found: {path}')\n",
        "    import pandas as pd\n",
        "    return pd.DataFrame(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checker functions for each layer\n",
        "def check_bronze(path):\n",
        "    print('--- Bronze check ---')\n",
        "    try:\n",
        "        if IS_DATABRICKS:\n",
        "            df = read_delta_table(path)\n",
        "            cnt = df.count()\n",
        "            print('Bronze rows (files):', cnt)\n",
        "            display(df.limit(5))\n",
        "            if cnt == 0:\n",
        "                print('WARNING: Bronze table has 0 rows — check Autoloader and raw storage path')\n",
        "                return False\n",
        "            return True\n",
        "        else:\n",
        "            # local: expect text files in folder\n",
        "            import os\n",
        "            if os.path.isdir(path):\n",
        "                files = os.listdir(path)\n",
        "                print('Files found:', len(files))\n",
        "                print(files[:10])\n",
        "                if len(files) == 0:\n",
        "                    print('WARNING: No raw files found in local bronze folder')\n",
        "                    return False\n",
        "                return True\n",
        "            else:\n",
        "                print('Local bronze path not found:', path)\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print('Bronze check failed:', e)\n",
        "        return False\n",
        "\n",
        "def check_silver_raw(path):\n",
        "    print('--- Silver Raw (OCR) check ---')\n",
        "    try:\n",
        "        if IS_DATABRICKS:\n",
        "            df = read_delta_table(path)\n",
        "            cnt = df.count()\n",
        "            print('Silver Raw rows:', cnt)\n",
        "            display(df.select('filename','text_raw').limit(5))\n",
        "            if cnt == 0:\n",
        "                print('WARNING: Silver Raw is empty — check OCR step')\n",
        "                return False\n",
        "            return True\n",
        "        else:\n",
        "            # local parquet or text\n",
        "            import os\n",
        "            if os.path.isdir(path):\n",
        "                # try parquet\n",
        "                try:\n",
        "                    import pandas as pd\n",
        "                    df = read_parquet_local(path)\n",
        "                    print('Loaded local parquet rows:', len(df))\n",
        "                    print(df.head())\n",
        "                    return True\n",
        "                except Exception:\n",
        "                    # try text files\n",
        "                    try:\n",
        "                        df = read_text_files_local(path)\n",
        "                        print('Loaded local text files:', len(df))\n",
        "                        print(df.head())\n",
        "                        return True\n",
        "                    except Exception as e:\n",
        "                        print('No usable files in local silver raw path:', e)\n",
        "                        return False\n",
        "            else:\n",
        "                print('Local silver raw path not found:', path)\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print('Silver Raw check failed:', e)\n",
        "        return False\n",
        "\n",
        "def check_silver_clean(path):\n",
        "    print('--- Silver Clean check ---')\n",
        "    try:\n",
        "        if IS_DATABRICKS:\n",
        "            df = read_delta_table(path)\n",
        "            cnt = df.count()\n",
        "            print('Silver Clean rows:', cnt)\n",
        "            display(df.select('filename','text_clean').limit(5))\n",
        "            if cnt == 0:\n",
        "                print('WARNING: Silver Clean is empty — check cleaning & PII redaction')\n",
        "                return False\n",
        "            # sanity checks\n",
        "            nulls = df.filter((df.text_clean.isNull()) | (df.text_clean == '')).count()\n",
        "            print('Null/empty cleaned texts:', nulls)\n",
        "            if nulls > 0:\n",
        "                print('WARNING: Some cleaned texts are empty — inspect OCR and cleaning regex rules')\n",
        "            return True\n",
        "        else:\n",
        "            try:\n",
        "                df = read_parquet_local(path)\n",
        "                print('Local cleaned rows:', len(df))\n",
        "                print(df.head())\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print('Silver Clean local read failed:', e)\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print('Silver Clean check failed:', e)\n",
        "        return False\n",
        "\n",
        "def check_gold(path):\n",
        "    print('--- Gold check (clauses & features) ---')\n",
        "    try:\n",
        "        if IS_DATABRICKS:\n",
        "            df = read_delta_table(path)\n",
        "            cnt = df.count()\n",
        "            print('Gold rows:', cnt)\n",
        "            display(df.limit(10))\n",
        "            # expect columns: clause_text or embedding_mean/clause_count/num_negations\n",
        "            cols = df.columns\n",
        "            print('Columns:', cols)\n",
        "            if 'clause_text' in cols:\n",
        "                print('Found clause_text column — good for classifier')\n",
        "            else:\n",
        "                print('Missing clause_text column — classifier needs clause_text & label')\n",
        "            if 'embedding_mean' in cols and 'clause_count' in cols:\n",
        "                print('Found features columns — good for risk model')\n",
        "            else:\n",
        "                print('Missing feature columns for risk model — create features from clauses')\n",
        "            if cnt == 0:\n",
        "                print('WARNING: Gold table has 0 rows — segmentation produced no clauses')\n",
        "                return False\n",
        "            return True\n",
        "        else:\n",
        "            try:\n",
        "                df = read_parquet_local(path)\n",
        "                print('Local gold rows:', len(df))\n",
        "                print(df.head())\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print('Gold local read failed:', e)\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print('Gold check failed:', e)\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the checks with chosen paths\n",
        "def run_all_checks():\n",
        "    bronze_path = DEFAULT_BRONZE_PATH if IS_DATABRICKS else LOCAL_BRONZE_PATH\n",
        "    silver_raw_path = DEFAULT_SILVER_RAW_PATH if IS_DATABRICKS else LOCAL_SILVER_RAW_PATH\n",
        "    silver_clean_path = DEFAULT_SILVER_CLEAN_PATH if IS_DATABRICKS else LOCAL_SILVER_CLEAN_PATH\n",
        "    gold_path = DEFAULT_GOLD_PATH if IS_DATABRICKS else LOCAL_GOLD_PATH\n",
        "    print('\\nUsing paths:')\n",
        "    print('bronze_path ->', bronze_path)\n",
        "    print('silver_raw_path ->', silver_raw_path)\n",
        "    print('silver_clean_path ->', silver_clean_path)\n",
        "    print('gold_path ->', gold_path)\n",
        "    ok = True\n",
        "    ok &= check_bronze(bronze_path)\n",
        "    ok &= check_silver_raw(silver_raw_path)\n",
        "    ok &= check_silver_clean(silver_clean_path)\n",
        "    ok &= check_gold(gold_path)\n",
        "    print('\\nOverall pipeline health:', 'OK' if ok else 'ISSUES FOUND')\n",
        "    if not ok:\n",
        "        print('Read the warnings above and inspect the specific notebook where the failure occurred. Use the sample rows printed to reproduce the problem locally.')\n",
        "    return ok\n",
        "\n",
        "run_all_checks()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
